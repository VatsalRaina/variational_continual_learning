{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VCL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VatsalRaina/variational_continual_learning/blob/master/VCL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReeWWM_rqmI0"
      },
      "source": [
        "!pip -q install blitz-bayesian-pytorch\n",
        "!pip -q install transformers"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Kk7D9bxDeyh",
        "outputId": "ba285cd0-7f05-4804-f9aa-c095a1030d45"
      },
      "source": [
        "!wget \"https://github.com/VatsalRaina/variational_continual_learning/raw/main/discriminative/permutedMNIST/mnist.pkl.gz\""
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-11 09:50:49--  https://github.com/VatsalRaina/variational_continual_learning/raw/main/discriminative/permutedMNIST/mnist.pkl.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/VatsalRaina/variational_continual_learning/main/discriminative/permutedMNIST/mnist.pkl.gz [following]\n",
            "--2021-03-11 09:50:49--  https://raw.githubusercontent.com/VatsalRaina/variational_continual_learning/main/discriminative/permutedMNIST/mnist.pkl.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16168813 (15M) [application/octet-stream]\n",
            "Saving to: ‘mnist.pkl.gz.1’\n",
            "\n",
            "mnist.pkl.gz.1      100%[===================>]  15.42M  37.7MB/s    in 0.4s    \n",
            "\n",
            "2021-03-11 09:50:49 (37.7 MB/s) - ‘mnist.pkl.gz.1’ saved [16168813/16168813]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwieNqjstX-j"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "class VCL_layer(torch.nn.Module):\n",
        "    def __init__(self, input_size: int, output_size: int, init_variance: float, previous_W_b:None):\n",
        "        super().__init__()\n",
        "        self.epsilon = 1e-8\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.init_variance = init_variance\n",
        "\n",
        "        if previous_W_b !=None:\n",
        "            self.register_buffer('prior_W_mean', torch.from_numpy(previous_W_b[0])) # Reversed dimensions for torch.nn.functional.linear\n",
        "            self.register_buffer('prior_b_mean', torch.from_numpy(previous_W_b[1]))\n",
        "        else:\n",
        "            self.register_buffer('prior_W_mean', torch.randn((output_size, input_size))/10) # Reversed dimensions for torch.nn.functional.linear\n",
        "            self.register_buffer('prior_b_mean', torch.randn(output_size)/10)\n",
        "        self.register_buffer('prior_W_logvar', torch.ones(self.output_size,self.input_size)*init_variance)\n",
        "        self.register_buffer('prior_b_logvar', torch.ones(self.output_size)*init_variance)\n",
        "\n",
        "        if previous_W_b !=None:\n",
        "            self.posterior_W_mean = Parameter(torch.tensor(previous_W_b[0], requires_grad=True))\n",
        "            self.posterior_b_mean = Parameter(torch.tensor(previous_W_b[1], requires_grad=True))\n",
        "        else:\n",
        "            self.posterior_W_mean = Parameter(torch.randn((output_size, input_size), requires_grad=True)/10)\n",
        "            self.posterior_b_mean = Parameter(torch.randn((output_size), requires_grad=True)/10)\n",
        "        self.posterior_W_logvar = Parameter(torch.nn.init.constant(torch.empty((self.output_size,self.input_size), requires_grad=True), math.log(self.init_variance)))\n",
        "        self.posterior_b_logvar = Parameter(torch.nn.init.constant(torch.empty(self.output_size,requires_grad=True), math.log(self.init_variance)))\n",
        "        self.register_parameter('posterior_W_mean', self.posterior_W_mean)\n",
        "        self.register_parameter('posterior_b_mean', self.posterior_b_mean)\n",
        "        self.register_parameter('posterior_W_logvar', self.posterior_W_logvar)\n",
        "        self.register_parameter('posterior_b_logvar', self.posterior_b_logvar)\n",
        "\n",
        "    def sample_parameters(self):\n",
        "        epsilon_W = torch.randn_like(self.posterior_W_mean)\n",
        "        epsilon_b = torch.randn_like(self.posterior_b_mean)\n",
        "        W_sample = self.posterior_W_mean + epsilon_W * torch.exp(0.5 * self.posterior_W_logvar) # Element-wise multiplication of epsilon with variance\n",
        "        b_sample = self.posterior_b_mean + epsilon_b * torch.exp(0.5 * self.posterior_b_logvar) \n",
        "        return W_sample, b_sample\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, b = self.sample_parameters()\n",
        "        return torch.nn.functional.linear(x, W, b) # No activation function here, will be managed in main model\n",
        "\n",
        "    def kl_divergence(self):\n",
        "        #TODO: redo the demonstration of this\n",
        "        prior_means = torch.autograd.Variable(torch.cat(\n",
        "            (torch.reshape(self.prior_W_mean, (-1,)),\n",
        "             torch.reshape(self.prior_b_mean, (-1,)))),\n",
        "            requires_grad=False\n",
        "        )\n",
        "        prior_logvars = torch.autograd.Variable(torch.cat(\n",
        "            (torch.reshape(self.prior_W_logvar, (-1,)),\n",
        "             torch.reshape(self.prior_b_logvar, (-1,)))),\n",
        "            requires_grad=False\n",
        "        )\n",
        "        prior_vars = torch.exp(prior_logvars)\n",
        "\n",
        "        posterior_means = torch.cat(\n",
        "            (torch.reshape(self.posterior_W_mean, (-1,)),\n",
        "             torch.reshape(self.posterior_b_mean, (-1,))),\n",
        "        )\n",
        "        posterior_logvars = torch.cat(\n",
        "            (torch.reshape(self.posterior_W_logvar, (-1,)),\n",
        "             torch.reshape(self.posterior_b_logvar, (-1,))),\n",
        "        )\n",
        "        posterior_vars = torch.exp(posterior_logvars)\n",
        "\n",
        "        # compute kl divergence (this computation is valid for multivariate diagonal Gaussians)\n",
        "        kl_elementwise = posterior_vars / (prior_vars + self.epsilon) + \\\n",
        "                         torch.pow(prior_means - posterior_means, 2) / (prior_vars + self.epsilon) - \\\n",
        "                         1 + prior_logvars - posterior_logvars\n",
        "        return 0.5 * kl_elementwise.sum()\n",
        "    \n",
        "    def update_prior_posterior(self):\n",
        "        \"\"\"The previous posterior becomes the new prior\"\"\"\n",
        "        self._buffers['prior_W_mean'].data.copy_(self.posterior_W_mean.data)\n",
        "        self._buffers['prior_W_logvar'].data.copy_(self.posterior_W_logvar.data)\n",
        "        self._buffers['prior_b_mean'].data.copy_(self.posterior_b_mean.data)\n",
        "        self._buffers['prior_b_logvar'].data.copy_(self.posterior_b_logvar.data)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyUC9Dlkru5e"
      },
      "source": [
        "Class to store VCL models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyi9BPPoqxIT"
      },
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "from blitz.modules import BayesianLinear\n",
        "from blitz.utils import variational_estimator\n",
        "\n",
        "class Vanilla_NN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_hidden_layers=2):\n",
        "\n",
        "        super(Vanilla_NN, self).__init__()\n",
        "\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.input_layer = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.hidden_layers = []\n",
        "        self.n_hidden_layers = n_hidden_layers\n",
        "        for i in range(n_hidden_layers):\n",
        "            self.hidden_layers.append(torch.nn.Linear(hidden_dim, hidden_dim))\n",
        "        self.output_layer = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h1 = self.relu(self.input_layer(x))\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            h2 = self.relu(hidden_layer(h1))\n",
        "            h1=h2\n",
        "        prediction_logits = self.output_layer(h2)\n",
        "        \n",
        "        return prediction_logits\n",
        "\n",
        "    \n",
        "    def get_parameters(self):\n",
        "        \"\"\" Returns model weights and biases as a dictionnary: \n",
        "        dic['input'] = (W_mean_input, b_mean_input)\n",
        "        \"\"\"\n",
        "        input_W_b = (self.input_layer.weight.detach().numpy(),self.input_layer.bias.detach().numpy())\n",
        "        layers_W_b = [(self.hidden_layers[i].weight.detach().numpy(),self.hidden_layers[i].bias.detach().numpy()) for i in range(self.n_hidden_layers)]\n",
        "        output_W_b = (self.output_layer.weight.detach().numpy(),self.output_layer.bias.detach().numpy())\n",
        "        dic = {'input':input_W_b, 'layers':layers_W_b, 'output':output_W_b}\n",
        "        return dic\n",
        "\n",
        "\n",
        "\n",
        "# See explanation at:\n",
        "# https://towardsdatascience.com/blitz-a-bayesian-neural-network-library-for-pytorch-82f9998916c7\n",
        "# Original code at:\n",
        "# https://github.com/piEsposito/blitz-bayesian-deep-learning \n",
        "@variational_estimator\n",
        "class MFVI_NN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_heads, prev_weights, n_hidden_layers=2):\n",
        "\n",
        "        super(MFVI_NN, self).__init__()\n",
        "\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.input_layer = BayesianLinear(input_dim, hidden_dim)\n",
        "        self.hidden_layers = []\n",
        "        for i in range(n_hidden_layers):\n",
        "            self.hidden_layers.append(BayesianLinear(hidden_dim, hidden_dim))\n",
        "        self.heads = []\n",
        "        for i in range(n_heads):\n",
        "            self.heads.append(BayesianLinear(hidden_dim, output_dim))\n",
        "\n",
        "        # Initialise using the Vanilla neural network weights when the model is first initialised\n",
        "        # self.init_weights(prev_weights)\n",
        "\n",
        "    def init_weights(self, prev_weights):\n",
        "        \"\"\"\n",
        "        Initialise using Vanilla neural netwrok parameters for the means and a pre-decided variance\n",
        "        \"\"\"\n",
        "        # Instead of initialising using Vanilla NN, this model can be trained with all the variances for the first task\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, task):\n",
        "\n",
        "        h1 = self.relu(self.input_layer(x))\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            h2 = self.relu(hidden_layer(h1))\n",
        "            h1=h2\n",
        "        prediction_logits = self.heads[task](h2)\n",
        "        \n",
        "        return prediction_logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class VCL_discriminative(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_heads, prev_weights, n_hidden_layers=3):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_hidden_layers = n_hidden_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.init_variance = 1e-3\n",
        "\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.input_layer = VCL_layer(input_dim, hidden_dim, self.init_variance, previous_W_b=prev_weights['input'])\n",
        "        self.hidden_layers = torch.nn.ModuleList([VCL_layer(hidden_dim, hidden_dim, self.init_variance,previous_W_b=prev_weights['layers'][i]) for i in range(n_hidden_layers)])\n",
        "\n",
        "        self.heads = torch.nn.ModuleList([VCL_layer(hidden_dim, output_dim, self.init_variance, previous_W_b=prev_weights['output']) for _ in range(n_heads)])\n",
        "\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "        return\n",
        "\n",
        "    def forward(self, x, task:int):\n",
        "        x = self.relu(self.input_layer(x))\n",
        "        for layer in self.hidden_layers:\n",
        "            x = self.relu(layer(x))\n",
        "        x = self.softmax(self.heads[task](x))\n",
        "        return x\n",
        "\n",
        "    def vcl_loss(self, x, y, task):\n",
        "        return self.kl_divergence(task) - torch.nn.NLLLoss()(self(x, task), y)\n",
        "\n",
        "    def kl_divergence(self, task:int):\n",
        "        div = torch.zeros(1, requires_grad=False)\n",
        "        for layer in self.hidden_layers:\n",
        "            div = torch.add(div, layer.kl_divergence())\n",
        "        div = torch.add(div, self.heads[task].kl_divergence())\n",
        "        return div\n",
        "\n",
        "    def update_prior_posterior(self, head:int):\n",
        "        for layer in self.hidden_layers:\n",
        "            layer.update_prior_posterior()\n",
        "        self.heads[head].update_prior_posterior()\n",
        "        return\n",
        "\n",
        "    def prediction(self, x, head:int):\n",
        "        return torch.argmax(self(x, task))\n"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGW1t6BisPTx"
      },
      "source": [
        ""
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-flzXBquhf5"
      },
      "source": [
        "import gzip\n",
        "import pickle \n",
        "from copy import deepcopy\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class PermutedMnistGenerator():\n",
        "    def __init__(self, num_tasks=10):\n",
        "        #Unzipping and reading Compressex MNIST DATA\n",
        "        f = gzip.open('mnist.pkl.gz', 'rb')\n",
        "        u = pickle._Unpickler( f )\n",
        "        u.encoding = 'latin1'\n",
        "        train_set, valid_set, test_set = u.load()\n",
        "        f.close()\n",
        "\n",
        "        self.X_train = np.vstack((train_set[0], valid_set[0]))\n",
        "        self.Y_train = np.hstack((train_set[1], valid_set[1]))\n",
        "        self.X_test = test_set[0]\n",
        "        self.Y_test = test_set[1]\n",
        "\n",
        "    def create_tasks(self, num_tasks=10):\n",
        "        np.random.seed(0)\n",
        "\n",
        "        X_train, Y_train, X_test, Y_test = [], [], [] ,[]\n",
        "\n",
        "        for i in range(num_tasks):\n",
        "            x_train, y_train, x_test, y_test = self.generate_new_task()\n",
        "            X_train.append(x_train)\n",
        "            Y_train.append(y_train)\n",
        "            X_test.append(x_test)\n",
        "            Y_test.append(y_test)\n",
        "\n",
        "        return (X_train, Y_train, X_test, Y_test)\n",
        "\n",
        "    def print_example(self, examples=[0]):\n",
        "        for example in examples:\n",
        "            array = self.X_train[example]\n",
        "            array_2D = np.reshape(array, (28, 28))\n",
        "            img = Image.fromarray(np.uint8(array_2D * 255) , 'L')\n",
        "            img.show()\n",
        "\n",
        "    def generate_new_task(self):\n",
        "        perm_inds = list(range(self.X_train.shape[1]))\n",
        "        np.random.shuffle(perm_inds)\n",
        "\n",
        "        # Retrieve train data\n",
        "        x_train = deepcopy(self.X_train)\n",
        "        x_train = x_train[:,perm_inds]\n",
        "        # y_train = np.eye(10)[self.Y_train]   #One hot encodes labels\n",
        "        y_train = self.Y_train\n",
        "\n",
        "        # Retrieve test data\n",
        "        x_test = deepcopy(self.X_test)\n",
        "        x_test = x_test[:,perm_inds]\n",
        "        # y_test = np.eye(10)[self.Y_test]\n",
        "        y_test = self.Y_test\n",
        "\n",
        "        return x_train, y_train, x_test, y_test\n"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZrkchQ2wqfy"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "def get_default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Got CUDA!\")\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "############################ Handle to use fron scrach model or not ######################\n",
        "###########################################################################################\n",
        "use_from_scratch_model = use_from_scratch_model\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = seed\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Choose device\n",
        "device = get_default_device()\n"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT0yLueBw-5-"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "data_processor = PermutedMnistGenerator(num_tasks)\n",
        "X_train, Y_train, X_test, Y_test = data_processor.create_tasks(num_tasks)\n",
        "x_train, y_train = torch.tensor(X_train[0]).to(device), torch.tensor(Y_train[0]).long().to(device)\n",
        "train_data = TensorDataset(x_train, y_train)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp0ReI2jsPxC"
      },
      "source": [
        "hidden_size=100\n",
        "n_hidden=2\n",
        "batch_size=256\n",
        "no_epochs=100\n",
        "num_tasks=5\n",
        "coreset_size=0\n",
        "adam_epsilon=1e-8\n",
        "learning_rate=2e-3\n",
        "use_from_scratch_model=True\n",
        "seed = 2"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJRojNLozL1j",
        "outputId": "31be0efb-04f6-448f-f133-07a176c05ab1"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "################## Train Vanilla_NN using data for first task ######################\n",
        "\n",
        "vanilla_model = Vanilla_NN(input_dim=x_train.size()[1], hidden_dim=hidden_size, output_dim=10, n_hidden_layers=n_hidden).to(device)\n",
        "mf_weights = vanilla_model.get_parameters()\n",
        "\n",
        "optimizer = AdamW(vanilla_model.parameters(),\n",
        "                lr = learning_rate,\n",
        "                eps = adam_epsilon\n",
        "                )\n",
        "loss_values = []\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(5):\n",
        "    print(f\"\\n ======== Epoch {epoch + 1} / {no_epochs} ========\")\n",
        "    print('Training...')\n",
        "    t0 = time.time()\n",
        "    total_loss = 0\n",
        "    vanilla_model.train()\n",
        "    vanilla_model.zero_grad()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "        b_x = batch[0].to(device)\n",
        "        b_y = batch[1].to(device)\n",
        "        vanilla_model.zero_grad()\n",
        "        logits = vanilla_model(b_x)\n",
        "        loss = criterion(logits, b_y)\n",
        "        total_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(vanilla_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "vanilla_weights = vanilla_model.get_parameters()"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ======== Epoch 1 / 100 ========\n",
            "Training...\n",
            "  Batch    40  of    235.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    235.    Elapsed: 0:00:01.\n",
            "  Batch   120  of    235.    Elapsed: 0:00:01.\n",
            "  Batch   160  of    235.    Elapsed: 0:00:02.\n",
            "  Batch   200  of    235.    Elapsed: 0:00:02.\n",
            "\n",
            "  Average training loss: 0.64\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            " ======== Epoch 2 / 100 ========\n",
            "Training...\n",
            "  Batch    40  of    235.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    235.    Elapsed: 0:00:01.\n",
            "  Batch   120  of    235.    Elapsed: 0:00:01.\n",
            "  Batch   160  of    235.    Elapsed: 0:00:02.\n",
            "  Batch   200  of    235.    Elapsed: 0:00:02.\n",
            "\n",
            "  Average training loss: 0.23\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            " ======== Epoch 3 / 100 ========\n",
            "Training...\n",
            "  Batch    40  of    235.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    235.    Elapsed: 0:00:01.\n",
            "  Batch   120  of    235.    Elapsed: 0:00:01.\n",
            "  Batch   160  of    235.    Elapsed: 0:00:02.\n",
            "  Batch   200  of    235.    Elapsed: 0:00:02.\n",
            "\n",
            "  Average training loss: 0.18\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            " ======== Epoch 4 / 100 ========\n",
            "Training...\n",
            "  Batch    40  of    235.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    235.    Elapsed: 0:00:01.\n",
            "  Batch   120  of    235.    Elapsed: 0:00:01.\n",
            "  Batch   160  of    235.    Elapsed: 0:00:02.\n",
            "  Batch   200  of    235.    Elapsed: 0:00:02.\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            " ======== Epoch 5 / 100 ========\n",
            "Training...\n",
            "  Batch    40  of    235.    Elapsed: 0:00:00.\n",
            "  Batch    80  of    235.    Elapsed: 0:00:01.\n",
            "  Batch   120  of    235.    Elapsed: 0:00:01.\n",
            "  Batch   160  of    235.    Elapsed: 0:00:02.\n",
            "  Batch   200  of    235.    Elapsed: 0:00:02.\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epoch took: 0:00:02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "xxjWRespsPzv",
        "outputId": "d1c11605-2b5d-42bb-cddc-f82b38da6bf2"
      },
      "source": [
        "# Now we are at a stage where we can extract the weights from the above trained model and call them the W_means and b_means\n",
        "init_weights = vanilla_weights\n",
        "init_weights = {'input':None, 'layers':[None]*n_hidden, 'output':None}\n",
        "\n",
        "################## Train MFVI NN #######################\n",
        "\n",
        "if use_from_scratch_model:\n",
        "    model = VCL_discriminative(input_dim = x_train.size()[1], hidden_dim=hidden_size, output_dim=10, \\\n",
        "                               n_heads=num_tasks, prev_weights=init_weights, n_hidden_layers=n_hidden).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr = learning_rate, eps = adam_epsilon)\n",
        "\n",
        "else:\n",
        "    model = MFVI_NN(in_dim=x_train.size()[1], hidden_dim=hidden_size, out_dim=10, num_tasks=num_tasks, \\\n",
        "                    prev_weights=vanilla_weights, n_hidden_layers=n_hidden).to(device)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "for task_id in range(num_tasks):\n",
        "    # Extract task specific data\n",
        "    x_train, y_train = torch.tensor(X_train[task_id]).to(device), torch.tensor(Y_train[task_id]).long().to(device)\n",
        "    train_data = TensorDataset(x_train, y_train)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    # Set optimizer to be equal to all the shared parameters and the task specific head's parameters\n",
        "    if not use_from_scratch_model:\n",
        "        parameters = []\n",
        "        parameters.extend(model.inputLayer.parameters())\n",
        "        for hiddenlayer in model.hiddenLayers:\n",
        "            parameters.extend(hiddenlayer.parameters())\n",
        "\n",
        "        parameters.extend(model.outputHeads[task_id].parameters())\n",
        "        optimizer = AdamW(parameters, lr = learning_rate, eps = adam_epsilon)\n",
        "    loss_values = []\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(no_epochs):\n",
        "        total_loss = 0\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            b_x = batch[0].to(device)\n",
        "            b_y = batch[1].to(device)\n",
        "            model.zero_grad()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if not use_from_scratch_model:\n",
        "                prediction_logits = model(b_x, task_id)\n",
        "                fit_loss = criterion(prediction_logits, b_y)\n",
        "                # This is an inbuilt function for the imported BNN\n",
        "                # However, this KL term is finding the KL divergence between the setting of parameters in the current and previous mini-batch\n",
        "                # We are actually interested in finding the KL divergence between the setting of the parameters in the current mini-batch \n",
        "                # and the the final setting of the parameters from the previous TASK\n",
        "                # So we will need to write our own KL divergence function which finds KL only for the shared parameters\n",
        "                complexity_loss = model.nn_kl_divergence()  \n",
        "                loss = fit_loss + complexity_loss\n",
        "            \n",
        "            if use_from_scratch_model:\n",
        "                loss = model.vcl_loss(b_x, b_y, task_id)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        loss_values.append(avg_train_loss)\n",
        "        print(f\"\\n  Average training loss: {avg_train_loss:.2f}\")\n",
        "\n",
        "    # Now perform evaluation on the test data\n",
        "    x_test, y_test = X_test[task_id], Y_test[task_id]\n",
        "    #TODO "
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  Average training loss: 67539.39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-139-42b4c4406d03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvcl_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3VMXe-htiPA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}